"""
å¯¼å…¥é’»å‰å·¥ç¨‹æ—¥æŠ¥æ•°æ®ï¼ˆdrilling_pre_daily.xlsxï¼‰åˆ°PostgreSQLæ•°æ®åº“
"""

import pandas as pd
import psycopg2
from psycopg2 import sql, extras
from pathlib import Path
from datetime import datetime

# æ•°æ®åº“é…ç½®
DB_CONFIG = {
    'host': 'localhost',
    'port': 5432,
    'database': 'rag',
    'user': 'postgres',
    'password': 'postgres'
}

# Excelæ–‡ä»¶è·¯å¾„
EXCEL_FILE = "drilling_pre_daily.xlsx"

# å­—æ®µæ˜ å°„ï¼šExcelåˆ—å -> æ•°æ®åº“å­—æ®µå
# ç¬¬0è¡Œæ˜¯ä¸­æ–‡åï¼Œç¬¬1è¡Œæ˜¯è‹±æ–‡åï¼ˆä½œä¸ºåˆ—åï¼‰
COLUMN_MAPPING = {
    'KTXM': 'ktxm',           # å‹˜æ¢é¡¹ç›®
    'SSND': 'ssnd',           # å®æ–½å¹´åº¦
    'JH': 'jh',               # äº•å·
    'JWZYSJ': 'jwzysj',       # äº•ä½è®ºè¯æ—¶é—´
    'JWTJXDSJ': 'jwtjxdsj',   # äº•ä½æ¡ä»¶ä¸‹è¾¾æ—¶é—´
    'JWTCLSJ': 'jwtclsj',     # äº•ä½æµ‹é‡æ—¶é—´
    'TZXDSJ': 'tzxdsj',       # æŠ•èµ„ä¸‹è¾¾æ—¶é—´
    'KJCGCWSJ': 'kjcgcwsj',   # å‹˜ç•Œæˆæœå®Œæˆæ—¶é—´
    'HPSBSJ': 'hpsbsj',       # ç¯è¯„ä¸ŠæŠ¥æ—¶é—´
    'YDSQSBSJ': 'ydsqsbsj',   # ç”¨åœ°ç”³è¯·ä¸ŠæŠ¥æ—¶é—´
    'GCFATLSJ': 'gcfatlsj',   # å·¥ç¨‹æ–¹æ¡ˆè®¨è®ºæ—¶é—´
    'ZJDZSJSPSJ': 'zjdzsjspsj', # é’»äº•åœ°è´¨è®¾è®¡å®¡æ‰¹æ—¶é—´
    'ZJGCSJSPSJ': 'zjgcsjspsj', # é’»äº•å·¥ç¨‹è®¾è®¡å®¡æ‰¹æ—¶é—´
    'HPXDSJ': 'hpxdsj',       # ç¯è¯„ä¸‹è¾¾æ—¶é—´
    'ZDCWSJ': 'zdcwsj',       # å¾åœ°å®Œæˆæ—¶é—´
    'TLSKSJ': 'tlsksj',       # æ¢ä¸´å¼€å§‹æ—¶é—´
    'TLJSSJ': 'tljssj',       # æ¢ä¸´ç»“æŸæ—¶é—´
    'BJKSSJ': 'bjkssj',       # æ¬å®¶å®‰è£…å¼€å§‹æ—¶é—´
    'BJJSSJ': 'bjjssj',       # æ¬å®¶å®‰è£…ç»“æŸæ—¶é—´
}


def read_excel_data(file_path):
    """è¯»å–Excelæ–‡ä»¶"""
    print(f"ğŸ“„ è¯»å–Excelæ–‡ä»¶: {file_path}")
    
    if not Path(file_path).exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return None
    
    try:
        # ç¬¬1è¡Œï¼ˆç´¢å¼•1ï¼‰ä½œä¸ºè¡¨å¤´ï¼ˆè‹±æ–‡åˆ—åï¼‰
        df = pd.read_excel(file_path, header=1)
        
        # å»é™¤åˆ—åç©ºæ ¼
        df.columns = df.columns.str.strip()
        
        # åˆ é™¤ç¬¬ä¸€åˆ—ï¼ˆå¦‚æœæ˜¯Unnamedæˆ–NaNï¼‰
        if 'Unnamed: 0' in df.columns:
            df = df.drop(columns=['Unnamed: 0'])
        
        # å¦‚æœç¬¬ä¸€åˆ—å…¨æ˜¯NaNï¼Œåˆ é™¤
        if df.columns[0] and pd.isna(df.columns[0]):
            df = df.iloc[:, 1:]
        
        print(f"âœ“ æˆåŠŸè¯»å– {len(df)} è¡Œæ•°æ®")
        print(f"âœ“ åˆ—å: {list(df.columns)}")
        
        return df
        
    except Exception as e:
        print(f"âŒ è¯»å–Excelå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None


def clean_and_validate_data(df):
    """æ¸…æ´—å’ŒéªŒè¯æ•°æ®"""
    print("\nğŸ§¹ æ¸…æ´—å’ŒéªŒè¯æ•°æ®...")
    
    original_count = len(df)
    
    # åˆ é™¤äº•å·ä¸ºç©ºçš„è¡Œ
    before_jh = len(df)
    df = df.dropna(subset=['JH'])
    if before_jh != len(df):
        print(f"  - åˆ é™¤äº•å·ä¸ºç©ºçš„è¡Œ: {before_jh - len(df)} è¡Œ")
    
    # åˆ é™¤äº•å·å‰åç©ºæ ¼
    df['JH'] = df['JH'].astype(str).str.strip()
    
    # å¤„ç†å¹´åº¦å­—æ®µ
    if 'SSND' in df.columns:
        df['SSND'] = pd.to_numeric(df['SSND'], errors='coerce')
    
    # å¤„ç†æ‰€æœ‰æ—¥æœŸå­—æ®µ
    date_columns = [col for col in df.columns if col in COLUMN_MAPPING and col.endswith('SJ')]
    for col in date_columns:
        if col in df.columns:
            df[col] = pd.to_datetime(df[col], errors='coerce')
    
    print(f"âœ“ æ¸…æ´—åä¿ç•™ {len(df)} è¡Œæ•°æ®ï¼ˆåˆ é™¤äº† {original_count - len(df)} è¡Œï¼‰")
    
    return df


def create_table(conn):
    """åˆ›å»ºæ•°æ®è¡¨"""
    print("\nğŸ“‹ åˆ›å»ºæ•°æ®è¡¨...")
    
    try:
        with conn.cursor() as cur:
            # è¯»å–å¹¶æ‰§è¡Œschemaæ–‡ä»¶
            schema_file = Path("drilling_pre_daily_schema.sql")
            if schema_file.exists():
                with open(schema_file, 'r', encoding='utf-8') as f:
                    schema_sql = f.read()
                cur.execute(schema_sql)
                conn.commit()
                print("âœ“ æ•°æ®è¡¨åˆ›å»ºæˆåŠŸ")
            else:
                print("âŒ Schemaæ–‡ä»¶ä¸å­˜åœ¨")
                return False
                
    except Exception as e:
        print(f"âŒ åˆ›å»ºæ•°æ®è¡¨å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    return True


def clear_table(conn):
    """æ¸…ç©ºè¡¨æ•°æ®ï¼ˆå¯é€‰ï¼‰"""
    print("\nğŸ—‘ï¸  æ¸…ç©ºç°æœ‰æ•°æ®...")
    
    try:
        with conn.cursor() as cur:
            cur.execute("DELETE FROM drilling_pre_daily")
            conn.commit()
            print("âœ“ è¡¨æ•°æ®å·²æ¸…ç©º")
    except Exception as e:
        print(f"âŒ æ¸…ç©ºè¡¨æ•°æ®å¤±è´¥: {e}")
        return False
    
    return True


def import_data(conn, df):
    """å¯¼å…¥æ•°æ®åˆ°æ•°æ®åº“"""
    print(f"\nğŸ“¥ å¯¼å…¥æ•°æ®åˆ°æ•°æ®åº“...")
    
    try:
        # å‡†å¤‡æ’å…¥æ•°æ®
        records = []
        for idx, row in df.iterrows():
            record = {}
            for excel_col, db_col in COLUMN_MAPPING.items():
                if excel_col in df.columns:
                    value = row[excel_col]
                    # å¤„ç†NaNå€¼
                    if pd.isna(value):
                        record[db_col] = None
                    # å¤„ç†Timestampç±»å‹
                    elif isinstance(value, pd.Timestamp):
                        record[db_col] = value.to_pydatetime()
                    else:
                        record[db_col] = value
            records.append(record)
        
        # æ„å»ºæ’å…¥SQL
        if records:
            columns = list(records[0].keys())
            insert_query = sql.SQL(
                "INSERT INTO drilling_pre_daily ({}) VALUES ({})"
            ).format(
                sql.SQL(', ').join(map(sql.Identifier, columns)),
                sql.SQL(', ').join(sql.Placeholder() * len(columns))
            )
            
            # æ‰¹é‡æ’å…¥
            with conn.cursor() as cur:
                for record in records:
                    values = [record[col] for col in columns]
                    cur.execute(insert_query, values)
            
            conn.commit()
            print(f"âœ“ æˆåŠŸå¯¼å…¥ {len(records)} æ¡è®°å½•")
            return len(records)
        else:
            print("âš ï¸  æ²¡æœ‰æ•°æ®å¯å¯¼å…¥")
            return 0
            
    except Exception as e:
        conn.rollback()
        print(f"âŒ å¯¼å…¥æ•°æ®å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 0


def check_unmatched_wells(conn):
    """æ£€æŸ¥å“ªäº›äº•å·åœ¨oil_wellsè¡¨ä¸­ä¸å­˜åœ¨"""
    print("\nğŸ” æ£€æŸ¥äº•å·åŒ¹é…æƒ…å†µ...")
    
    try:
        with conn.cursor() as cur:
            # æŸ¥è¯¢drilling_pre_dailyä¸­ä¸åœ¨oil_wellsä¸­çš„äº•å·
            query = """
                SELECT DISTINCT dpd.jh
                FROM drilling_pre_daily dpd
                LEFT JOIN oil_wells ow ON dpd.jh = ow.well_name
                WHERE ow.well_name IS NULL AND dpd.jh IS NOT NULL
                ORDER BY dpd.jh
            """
            cur.execute(query)
            unmatched_wells = cur.fetchall()
            
            if unmatched_wells:
                print(f"\nâŒ å‘ç° {len(unmatched_wells)} ä¸ªäº•å·åœ¨oil_wellsè¡¨ä¸­ä¸å­˜åœ¨:")
                print("=" * 60)
                for idx, (well_name,) in enumerate(unmatched_wells, 1):
                    print(f"{idx:3d}. {well_name}")
                print("=" * 60)
            else:
                print("âœ… æ‰€æœ‰äº•å·éƒ½åœ¨oil_wellsè¡¨ä¸­å­˜åœ¨ï¼")
            
            # ç»Ÿè®¡æ€»æ•°
            cur.execute("SELECT COUNT(DISTINCT jh) FROM drilling_pre_daily WHERE jh IS NOT NULL")
            total_wells = cur.fetchone()[0]
            
            cur.execute("SELECT COUNT(*) FROM drilling_pre_daily")
            total_records = cur.fetchone()[0]
            
            print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
            print(f"  - drilling_pre_daily æ€»è®°å½•æ•°: {total_records}")
            print(f"  - drilling_pre_daily ä¸åŒäº•å·æ•°: {total_wells}")
            print(f"  - æœªåŒ¹é…äº•å·æ•°: {len(unmatched_wells)}")
            print(f"  - åŒ¹é…ç‡: {(total_wells - len(unmatched_wells)) / total_wells * 100:.2f}%")
            
            return unmatched_wells
            
    except Exception as e:
        print(f"âŒ æ£€æŸ¥äº•å·åŒ¹é…å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 80)
    print("é’»å‰å·¥ç¨‹æ—¥æŠ¥æ•°æ®å¯¼å…¥å·¥å…·")
    print("=" * 80)
    
    # 1. è¯»å–Excelæ•°æ®
    df = read_excel_data(EXCEL_FILE)
    if df is None:
        return
    
    # 2. æ¸…æ´—å’ŒéªŒè¯æ•°æ®
    df = clean_and_validate_data(df)
    if df is None or len(df) == 0:
        print("âŒ æ²¡æœ‰æœ‰æ•ˆæ•°æ®å¯å¯¼å…¥")
        return
    
    # 3. è¿æ¥æ•°æ®åº“
    print("\nğŸ”Œ è¿æ¥æ•°æ®åº“...")
    try:
        conn = psycopg2.connect(**DB_CONFIG)
        print("âœ“ æ•°æ®åº“è¿æ¥æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
        return
    
    try:
        # 4. åˆ›å»ºæ•°æ®è¡¨
        if not create_table(conn):
            return
        
        # 5. æ¸…ç©ºç°æœ‰æ•°æ®ï¼ˆå¯é€‰ï¼‰
        user_input = input("\næ˜¯å¦æ¸…ç©ºç°æœ‰æ•°æ®ï¼Ÿ(y/n, é»˜è®¤y): ").strip().lower()
        if user_input != 'n':
            if not clear_table(conn):
                return
        
        # 6. å¯¼å…¥æ•°æ®
        imported_count = import_data(conn, df)
        
        if imported_count > 0:
            # 7. æ£€æŸ¥äº•å·åŒ¹é…æƒ…å†µ
            check_unmatched_wells(conn)
        
        print("\n" + "=" * 80)
        print("âœ… å¯¼å…¥å®Œæˆï¼")
        print("=" * 80)
        
    finally:
        conn.close()
        print("\nğŸ”’ æ•°æ®åº“è¿æ¥å·²å…³é—­")


if __name__ == "__main__":
    main()
